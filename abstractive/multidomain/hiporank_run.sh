#!/bin/bash

# mbart
for lang in bn en hi ml mr or pa ta; do rm -rf hiporank_${lang}_mbart_monolingual.log; python train.py --train_path multidomain_data/hiporank_output_data/${lang}/${lang}_train.json --val_path multidomain_data/hiporank_output_data/${lang}/${lang}_val.json --test_path multidomain_data/hiporank_output_data/${lang}/${lang}_test.json --tokenizer facebook/mbart-large-50 --model facebook/mbart-large-50 --is_mt5 0 --exp_name lang_wise_hiporank_${lang}_mbart-summ --save_dir ./ --num_epochs 1 --train_batch_size 4 --val_batch_size 4 --test_batch_size 4 --max_source_length 512 --max_target_length 512 --n_gpus 4 --strategy ddp --sanity_run no 2>&1|tee -a hiporank_${lang}_mbart_monolingual.log;done

# mt5
for lang in bn en hi ml mr or pa ta; do rm -rf hiporank_${lang}_mt5_monolingual.log; python train.py --train_path multidomain_data/hiporank_output_data/${lang}/${lang}_train.json --val_path multidomain_data/hiporank_output_data/${lang}/${lang}_val.json --test_path multidomain_data/hiporank_output_data/${lang}/${lang}_test.json --tokenizer google/mt5-base --model google/mt5-base --is_mt5 1 --exp_name lang_wise_hiporank_${lang}_mt5-summ --save_dir ./ --target_lang ${lang} --num_epochs 1 --train_batch_size 4 --val_batch_size 4 --test_batch_size 4 --max_source_length 512 --max_target_length 512 --n_gpus 4 --strategy ddp --sanity_run no 2>&1|tee -a hiporank_${lang}_mt5_monolingual.log;done
